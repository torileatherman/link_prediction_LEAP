{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, preprocess, and store data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104615cb0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Coauthor, Planetoid, WikipediaNetwork\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import homogeneous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://graphmining.ai/datasets/ptg/wiki/crocodile.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "root = '../data'\n",
    "wiki_datasets = [\"chameleon\",\"crocodile\"]\n",
    "planetoid_dataset = \"PubMed\"\n",
    "coauthor_dataset = \"CS\"\n",
    "\n",
    "wiki_chameleon = WikipediaNetwork(root=root, name=wiki_datasets[0]).data\n",
    "wiki_crocodile = WikipediaNetwork(root=root, name=wiki_datasets[1], geom_gcn_preprocess=False).data\n",
    "pubmed = Planetoid(root=root, name=planetoid_dataset).data\n",
    "cs = Coauthor(root=root, name=coauthor_dataset).data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split each dataset and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inductive_split(data: torch_geometric.data):\n",
    "    ''' \n",
    "    Function that takes graph data and creates train, test, and valid masks \n",
    "    in order to perform inductive link prediction.\n",
    "\n",
    "    When splitting wikipedia data, use split 0.8, 0.9\n",
    "    When splitting pubmed and cs, use split 0.30, 0.65\n",
    "    '''\n",
    "    rands = torch.rand(data.num_nodes)\n",
    "    \n",
    "    # Initialize size of splits\n",
    "    train_mask =  rands < 0.3\n",
    "    test_mask = rands > 0.65       \n",
    "    val_mask = []\n",
    "\n",
    "    #Create val_mask with nodes not in train_mask or test_mask\n",
    "    for i in torch.arange(data.num_nodes): \n",
    "        if (i not in train_mask.nonzero() and i not in test_mask.nonzero()):\n",
    "            val_mask.append(True)\n",
    "        else:\n",
    "            val_mask.append(False)\n",
    "            \n",
    "    val_mask  = torch.Tensor(val_mask).to(torch.bool)\n",
    "\n",
    "    # Create subgraphs based on node assignments in masks        \n",
    "    train_data = data.clone()\n",
    "    train_data.edge_index, _ = subgraph(train_mask, data.edge_index, relabel_nodes=True)\n",
    "    train_data.x = data.x[train_mask]\n",
    "\n",
    "    val_data = data.clone()\n",
    "    val_data.edge_index, _ = subgraph(val_mask, data.edge_index, relabel_nodes=True)\n",
    "    val_data.x = data.x[val_mask]\n",
    "            \n",
    "    test_data = data.clone()\n",
    "    test_data.edge_index, _ = subgraph(test_mask, data.edge_index, relabel_nodes=True)\n",
    "    test_data.x = data.x[test_mask]\n",
    "\n",
    "    # Save each train, test, val subgraph using pickling\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    names =['train_data_'+timestr,'valid_data_'+timestr,'test_data_'+timestr]\n",
    "    data = [train_data, val_data, test_data]\n",
    "    for data, name in zip(data,names):\n",
    "        pickle.dump(data, bz2.BZ2File('../data/{0}.p'.format(name),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = [wiki_chameleon, wiki_crocodile]\n",
    "other_data = [pubmed, cs]\n",
    "for set in other_data:\n",
    "    inductive_split(set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d031f367699a04ff1c380b258116486c512c2d8deb2bd9c991bfcd47444ad3b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
