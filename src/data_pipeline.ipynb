{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, preprocess, and store data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10c03c5d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric as tg\n",
    "import torch\n",
    "from torch_geometric.datasets import Coauthor, Planetoid, WikipediaNetwork\n",
    "from torch_geometric.transforms import RandomLinkSplit, RandomNodeSplit\n",
    "from torch_geometric.utils import subgraph\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "import pickle\n",
    "import bz2\n",
    "import time\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import homogeneous datasets and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_node_feature_label.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/new_data/chameleon/out1_graph_edges.txt\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_0.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_1.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_2.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_3.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_4.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_5.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_6.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_7.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_8.npz\n",
      "Downloading https://raw.githubusercontent.com/graphdml-uiuc-jlu/geom-gcn/f1fc0d14b3b019c562737240d06ec83b07d16a8f/splits/chameleon_split_0.6_0.2_9.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://graphmining.ai/datasets/ptg/wiki/crocodile.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.pubmed.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/shchur/gnn-benchmark/raw/master/data/npz/ms_academic_cs.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "root = '../data'\n",
    "wiki_datasets = [\"chameleon\",\"crocodile\"]\n",
    "planetoid_dataset = \"PubMed\"\n",
    "coauthor_dataset = \"CS\"\n",
    "\n",
    "wiki_chameleon = WikipediaNetwork(root=root, name=wiki_datasets[0]).data\n",
    "wiki_crocodile = WikipediaNetwork(root=root, name=wiki_datasets[1], geom_gcn_preprocess=False).data\n",
    "pubmed = Planetoid(root=root, name=planetoid_dataset).data\n",
    "cs = Coauthor(root=root, name=coauthor_dataset).data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split each dataset and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inductive_split(data: tg.data):\n",
    "    ''' \n",
    "    Function that takes graph data and creates train, test, and valid masks \n",
    "    in order to perform inductive link prediction.\n",
    "\n",
    "    When splitting wikipedia data, use split 0.8, 0.9\n",
    "    When splitting pubmed and cs, use split 0.30, 0.65\n",
    "    '''\n",
    "    # Create train / test / validation masks\n",
    "    rnp = RandomNodeSplit(num_val=0.1, num_test=0.1)\n",
    "\n",
    "    data = data.clone()\n",
    "    data = rnp(data)\n",
    "\n",
    "    # Create edges lists from train / test / validation masks from randomnodesplit\n",
    "    adj = torch.sparse_coo_tensor(data.edge_index, torch.ones(data.edge_index.shape[1]), (data.num_nodes, data.num_nodes)).to_dense()\n",
    "    training_edges = adj[data.train_mask.nonzero().view(-1), :][:, data.train_mask.nonzero().view(-1)]\n",
    "    val_edges  = adj[data.train_mask.nonzero().view(-1), :][:, data.val_mask.nonzero().view(-1)]\n",
    "    testing_edges  = adj[data.train_mask.nonzero().view(-1), :][:, data.test_mask.nonzero().view(-1)]\n",
    "\n",
    "    # Initialize Data objects\n",
    "    training_data = Data()\n",
    "    testing_data = Data()\n",
    "    valid_data = Data()\n",
    "\n",
    "    # Create Data objects from masks and edge lists\n",
    "    training_data.x = data.x[data.train_mask]\n",
    "    training_data.edge_index = training_edges.nonzero().T\n",
    "    testing_data.x = data.x[data.test_mask]\n",
    "    testing_data.edge_index = testing_edges.nonzero().T\n",
    "    valid_data.x = data.x[data.val_mask]\n",
    "    valid_data.edge_index = val_edges.nonzero().T\n",
    "\n",
    "    # Use RandomLinkSplit to create edge_label_index for each data object\n",
    "    rlp = RandomLinkSplit(num_val=0, num_test=0)\n",
    "    \n",
    "    training_data, _, _ = rlp(\n",
    "            Data(\n",
    "                x = training_data.x,\n",
    "                edge_index=training_data.edge_index,\n",
    "                num_nodes=training_data.num_nodes\n",
    "            )\n",
    "        )\n",
    "\n",
    "    testing_data, _, _ = rlp(\n",
    "            Data(\n",
    "                x = testing_data.x,\n",
    "                edge_index=testing_data.edge_index,\n",
    "                num_nodes=testing_data.num_nodes\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    valid_data, _, _ = rlp(\n",
    "            Data(\n",
    "                x = valid_data.x,\n",
    "                edge_index=valid_data.edge_index,\n",
    "                num_nodes=valid_data.num_nodes\n",
    "            )\n",
    "        )\n",
    "            \n",
    "    # Save each train, test, val subgraph using pickling\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    names =['train_data_'+timestr,'valid_data_'+timestr,'test_data_'+timestr]\n",
    "    data = [training_data, valid_data, testing_data]\n",
    "    for data, name in zip(data,names):\n",
    "        pickle.dump(data, bz2.BZ2File('../data/{0}.p'.format(name),'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [wiki_chameleon, wiki_crocodile, pubmed, cs]\n",
    "for set in datasets:\n",
    "    inductive_split(set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d031f367699a04ff1c380b258116486c512c2d8deb2bd9c991bfcd47444ad3b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
